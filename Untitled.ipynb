{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "882503d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836fa62f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "052b3b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e78d73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "765b715f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 1 captured and saved as image_0.jpg\n",
      "Image 2 captured and saved as image_1.jpg\n",
      "Image 3 captured and saved as image_2.jpg\n"
     ]
    }
   ],
   "source": [
    "#just captures images\n",
    "def capture_images_skip_frames(num_images, delay):\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    # Check if the camera opened successfully\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open camera\")\n",
    "        return\n",
    "\n",
    "\n",
    "    # Capture and save images skipping frames\n",
    "    for i in range(num_images):\n",
    "        \n",
    "\n",
    "        ret, frame = cap.read()  # Capture frame-by-frame\n",
    "        if not ret:\n",
    "            print(\"Error: Failed to capture image\")\n",
    "            break\n",
    "\n",
    "        # Save the captured image\n",
    "        filename = f\"image_{i}.jpg\"\n",
    "        cv2.imwrite(filename, frame)\n",
    "        print(f\"Image {i+1} captured and saved as {filename}\")\n",
    "\n",
    "        # Increment skip_counter for the next iteration\n",
    "        time.sleep(delay)\n",
    "\n",
    "    # Release the camera\n",
    "    cap.release()\n",
    "\n",
    "# Number of images to capture\n",
    "num_images = 3\n",
    "\n",
    "delay = 2 \n",
    "\n",
    "capture_images_skip_frames(num_images, delay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3814fdf4",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.9.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\objdetect\\src\\cascadedetect.cpp:1689: error: (-215:Assertion failed) !empty() in function 'cv::CascadeClassifier::detectMultiScale'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 27\u001b[0m\n\u001b[0;32m     24\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124macer1\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDownloads\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mai&ml\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mSign lang\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mimage_1.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Call the function to detect hands in the image\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m detect_hands(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124macer1\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDownloads\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mai&ml\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mSign lang\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mimage_1.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[16], line 12\u001b[0m, in \u001b[0;36mdetect_hands\u001b[1;34m(image_path)\u001b[0m\n\u001b[0;32m      9\u001b[0m gray \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(image, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2GRAY)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Detect hands in the image\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m hands \u001b[38;5;241m=\u001b[39m hand_cascade\u001b[38;5;241m.\u001b[39mdetectMultiScale(gray, \u001b[38;5;241m1.1\u001b[39m, \u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Draw rectangles around the detected hands\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (x, y, w, h) \u001b[38;5;129;01min\u001b[39;00m hands:\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.9.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\objdetect\\src\\cascadedetect.cpp:1689: error: (-215:Assertion failed) !empty() in function 'cv::CascadeClassifier::detectMultiScale'\n"
     ]
    }
   ],
   "source": [
    "# hand_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_hand.xml')\n",
    "\n",
    "# # Function to detect hands in an image\n",
    "# def detect_hands(image_path):\n",
    "#     # Read the image\n",
    "#     image = cv2.imread(image_path)\n",
    "    \n",
    "#     # Convert the image to grayscale\n",
    "#     gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "#     # Detect hands in the image\n",
    "#     hands = hand_cascade.detectMultiScale(gray, 1.1, 4)\n",
    "    \n",
    "#     # Draw rectangles around the detected hands\n",
    "#     for (x, y, w, h) in hands:\n",
    "#         cv2.rectangle(image, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "    \n",
    "#     # Display the image with detected hands\n",
    "#     cv2.imshow('Hands Detected', image)\n",
    "#     cv2.waitKey(0)\n",
    "#     cv2.destroyAllWindows()\n",
    "\n",
    "# # Path to the image\n",
    "# image_path = r\"C:\\Users\\acer1\\Downloads\\ai&ml\\Sign lang\\image_1.jpg\"\n",
    "\n",
    "# # Call the function to detect hands in the image\n",
    "# detect_hands(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "beec1526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/imagenet_class_index.json\n",
      "35363/35363 [==============================] - 0s 1us/step\n",
      "1: Band_Aid (0.52)\n",
      "2: sunglasses (0.15)\n",
      "3: syringe (0.08)\n"
     ]
    }
   ],
   "source": [
    "#base model which identifies objects in image with camera\n",
    "import numpy as np\n",
    "from tensorflow.keras.applications import MobileNet\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.mobilenet import preprocess_input, decode_predictions\n",
    "\n",
    "# Load MobileNet model pre-trained on ImageNet\n",
    "model = MobileNet(weights='imagenet')\n",
    "\n",
    "# Load an image file\n",
    "img_path = r\"C:\\Users\\acer1\\Downloads\\ai&ml\\Sign lang\\image_1.jpg\"\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "\n",
    "# Preprocess the image\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(x)\n",
    "\n",
    "# Decode the predictions\n",
    "decoded_predictions = decode_predictions(predictions, top=3)[0]\n",
    "\n",
    "# Print the top 3 predictions\n",
    "for i, (imagenet_id, label, score) in enumerate(decoded_predictions):\n",
    "    print(f\"{i+1}: {label} ({score:.2f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba306217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\acer1\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\acer1\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\acer1\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\normalization\\batch_normalization.py:979: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\acer1\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Found 42142 images belonging to 35 classes.\n",
      "Found 42142 images belonging to 35 classes.\n",
      "Found 0 images belonging to 35 classes.\n",
      "Epoch 1/5\n",
      "WARNING:tensorflow:From C:\\Users\\acer1\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\acer1\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "921/921 [==============================] - 773s 835ms/step - loss: 0.7162 - accuracy: 0.8662\n",
      "Epoch 2/5\n",
      "921/921 [==============================] - 527s 572ms/step - loss: 0.0322 - accuracy: 0.9970\n",
      "Epoch 3/5\n",
      "921/921 [==============================] - 474s 515ms/step - loss: 0.0146 - accuracy: 0.9976\n",
      "Epoch 4/5\n",
      "921/921 [==============================] - 463s 503ms/step - loss: 0.0141 - accuracy: 0.9964\n",
      "Epoch 5/5\n",
      "921/921 [==============================] - 463s 502ms/step - loss: 0.0169 - accuracy: 0.9957\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x179d0433b10>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#training the model\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.applications import MobileNet\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
    "\n",
    "# Parameters\n",
    "image_size = (224, 224)\n",
    "num_classes = 35  # 10 digits + 26 letters\n",
    "batch_size = 32\n",
    "epochs = 5\n",
    "\n",
    "# Load MobileNet model without top layers\n",
    "base_model = MobileNet(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Freeze base model layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add custom top layers for classification\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "# Load and preprocess the data\n",
    "data_generator = datagen.flow_from_directory(\n",
    "    'C:/Users/acer1/Downloads/ai&ml/Sign lang/sign lang img data',\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False  # Ensure data isn't shuffled so train-test split is consistent\n",
    ")\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_generator.filenames, data_generator.classes, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train generator\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    'C:/Users/acer1/Downloads/ai&ml/Sign lang/sign lang img data',\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False,\n",
    "    subset='training'  # Use the training subset of the data\n",
    ")\n",
    "\n",
    "# Test generator\n",
    "test_generator = datagen.flow_from_directory(\n",
    "    'C:/Users/acer1/Downloads/ai&ml/Sign lang/sign lang img data',\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False,\n",
    "    subset='validation'  # Use the validation subset of the data\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(X_train) // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=test_generator,\n",
    "    validation_steps=len(X_test) // batch_size\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "22239210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save(\"sign_language_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c853a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 1 captured and saved as image_0.jpg\n",
      "1/1 [==============================] - 1s 902ms/step\n",
      "Predicted Class: E\n",
      "Confidence: 0.46277955\n",
      "Image 2 captured and saved as image_1.jpg\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "Predicted Class: 3\n",
      "Confidence: 0.5022979\n",
      "Image 3 captured and saved as image_2.jpg\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "Predicted Class: 3\n",
      "Confidence: 0.7240892\n"
     ]
    }
   ],
   "source": [
    "#to click images and predict it\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import os\n",
    "\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# Specify the full path to the model file\n",
    "model_path = \"C:/Users/acer1/Downloads/ai&ml/Sign lang/sign_language_model.h5\"\n",
    "\n",
    "# Load the trained model\n",
    "model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "\n",
    "# Function to preprocess the captured image for prediction\n",
    "def preprocess_image(frame, target_size=(224, 224)):\n",
    "    img = cv2.resize(frame, target_size)\n",
    "    img_array = np.expand_dims(img, axis=0)\n",
    "    img_array = img_array / 255.0  # Normalize pixel values\n",
    "    return img_array\n",
    "\n",
    "# Function to make predictions on the captured image\n",
    "def predict_sign_language(frame):\n",
    "    preprocessed_image = preprocess_image(frame)\n",
    "    predictions = model.predict(preprocessed_image)\n",
    "    predicted_class_index = np.argmax(predictions)\n",
    "    #  mapping from class indices to class labels\n",
    "    class_labels = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n",
    "    predicted_class_label = class_labels[predicted_class_index]\n",
    "    return predicted_class_label, predictions[0][predicted_class_index]\n",
    "\n",
    "# Function to capture images skipping frames and make predictions\n",
    "def capture_and_predict_images(num_images, delay):\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    # Check if the camera opened successfully\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open camera\")\n",
    "        return\n",
    "\n",
    "    # Capture and predict images skipping frames\n",
    "    for i in range(num_images):\n",
    "        ret, frame = cap.read()  # Capture frame-by-frame\n",
    "        if not ret:\n",
    "            print(\"Error: Failed to capture image\")\n",
    "            break\n",
    "\n",
    "        # Save the captured image\n",
    "        filename = f\"image_{i}.jpg\"\n",
    "        cv2.imwrite(filename, frame)\n",
    "        print(f\"Image {i+1} captured and saved as {filename}\")\n",
    "\n",
    "        # Make prediction on the captured image\n",
    "        predicted_class, confidence = predict_sign_language(frame)\n",
    "        print(\"Predicted Class:\", predicted_class)\n",
    "        print(\"Confidence:\", confidence)\n",
    "\n",
    "        # Increment skip_counter for the next iteration\n",
    "        time.sleep(delay)\n",
    "\n",
    "    # Release the camera\n",
    "    cap.release()\n",
    "\n",
    "# Number of images to capture\n",
    "num_images = 3\n",
    "delay = 2 \n",
    "capture_and_predict_images(num_images, delay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da4fe52f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 1 captured and saved as image_0.jpg\n",
      "1/1 [==============================] - 1s 946ms/step\n",
      "Predicted Class: E\n",
      "Confidence: 0.2579836\n",
      "Image 2 captured and saved as image_1.jpg\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "Predicted Class: E\n",
      "Confidence: 0.2709978\n",
      "Image 3 captured and saved as image_2.jpg\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "Predicted Class: S\n",
      "Confidence: 0.3309207\n"
     ]
    }
   ],
   "source": [
    "#to click images and predict it\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# Specify the full path to the model file\n",
    "model_path = \"C:/Users/acer1/Downloads/ai&ml/Sign lang/sign_language_model.h5\"\n",
    "\n",
    "# Load the trained model only once\n",
    "model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "# Function to preprocess the captured image for prediction\n",
    "def preprocess_image(frame, target_size=(224, 224)):\n",
    "    img = cv2.resize(frame, target_size)\n",
    "    img_array = np.expand_dims(img, axis=0)\n",
    "    img_array = img_array / 255.0  # Normalize pixel values\n",
    "    return img_array\n",
    "\n",
    "# Function to make predictions on the captured image\n",
    "def predict_sign_language(frame):\n",
    "    preprocessed_image = preprocess_image(frame)\n",
    "    predictions = model.predict(preprocessed_image)\n",
    "    predicted_class_index = np.argmax(predictions)\n",
    "    #  mapping from class indices to class labels\n",
    "    class_labels = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n",
    "    predicted_class_label = class_labels[predicted_class_index]\n",
    "    return predicted_class_label, predictions[0][predicted_class_index]\n",
    "\n",
    "# Function to capture images skipping frames and make predictions\n",
    "def capture_and_predict_images(num_images, delay):\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    # Check if the camera opened successfully\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open camera\")\n",
    "        return\n",
    "\n",
    "    # Capture and predict images skipping frames\n",
    "    for i in range(num_images):\n",
    "        ret, frame = cap.read()  # Capture frame-by-frame\n",
    "        if not ret:\n",
    "            print(\"Error: Failed to capture image\")\n",
    "            break\n",
    "\n",
    "        # Save the captured image\n",
    "        filename = f\"image_{i}.jpg\"\n",
    "        cv2.imwrite(filename, frame)\n",
    "        print(f\"Image {i+1} captured and saved as {filename}\")\n",
    "\n",
    "        # Make prediction on the captured image\n",
    "        predicted_class, confidence = predict_sign_language(frame)\n",
    "        print(\"Predicted Class:\", predicted_class)\n",
    "        print(\"Confidence:\", confidence)\n",
    "\n",
    "        # Increment skip_counter for the next iteration\n",
    "        time.sleep(delay)\n",
    "\n",
    "    # Release the camera\n",
    "    cap.release()\n",
    "\n",
    "# Number of images to capture\n",
    "num_images = 3\n",
    "delay = 0.5  # Decrease delay for faster capture\n",
    "\n",
    "capture_and_predict_images(num_images, delay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f398de84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter class name (0-9, A-Z): 8\n",
      "Image saved as C:\\Users\\acer1\\Downloads\\ai&ml\\Sign lang\\sign lang img data\\8\\1219.jpg\n",
      "Enter class name (0-9, A-Z): 8\n",
      "Image saved as C:\\Users\\acer1\\Downloads\\ai&ml\\Sign lang\\sign lang img data\\8\\1220.jpg\n",
      "Enter class name (0-9, A-Z): 8\n",
      "Image saved as C:\\Users\\acer1\\Downloads\\ai&ml\\Sign lang\\sign lang img data\\8\\1221.jpg\n",
      "Enter class name (0-9, A-Z): 8\n",
      "Image saved as C:\\Users\\acer1\\Downloads\\ai&ml\\Sign lang\\sign lang img data\\8\\1222.jpg\n",
      "Enter class name (0-9, A-Z): 8\n",
      "Image saved as C:\\Users\\acer1\\Downloads\\ai&ml\\Sign lang\\sign lang img data\\8\\1223.jpg\n",
      "Enter class name (0-9, A-Z): 5\n",
      "Image saved as C:\\Users\\acer1\\Downloads\\ai&ml\\Sign lang\\sign lang img data\\5\\1220.jpg\n",
      "Enter class name (0-9, A-Z): 8\n",
      "Image saved as C:\\Users\\acer1\\Downloads\\ai&ml\\Sign lang\\sign lang img data\\8\\1224.jpg\n",
      "Enter class name (0-9, A-Z): 8\n",
      "Image saved as C:\\Users\\acer1\\Downloads\\ai&ml\\Sign lang\\sign lang img data\\8\\1225.jpg\n",
      "Enter class name (0-9, A-Z): 8\n",
      "Image saved as C:\\Users\\acer1\\Downloads\\ai&ml\\Sign lang\\sign lang img data\\8\\1226.jpg\n",
      "Enter class name (0-9, A-Z): 8\n",
      "Image saved as C:\\Users\\acer1\\Downloads\\ai&ml\\Sign lang\\sign lang img data\\8\\1227.jpg\n"
     ]
    }
   ],
   "source": [
    "#to save images to respective folders\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# Function to capture and save images\n",
    "def capture_and_save_images(num_images, data_dir):\n",
    "    # Initialize webcam\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    for i in range(num_images):\n",
    "        # Read a frame from the webcam\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Display the frame\n",
    "        cv2.imshow('Capture Image', frame)\n",
    "\n",
    "        # Prompt user to input the class name and folder name\n",
    "        class_name = input(\"Enter class name (0-9, A-Z): \")\n",
    "\n",
    "        # Check if the folder exists\n",
    "        folder_path = os.path.join(data_dir, class_name)\n",
    "        if not os.path.exists(folder_path):\n",
    "            print(f\"Error: Folder '{class_name}' does not exist.\")\n",
    "            continue\n",
    "\n",
    "        # Find the next available image number in the folder\n",
    "        image_files = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
    "        if image_files:\n",
    "            try:\n",
    "                image_numbers = [int(f.split('.')[0]) for f in image_files]\n",
    "                image_number = max(image_numbers) + 1\n",
    "            except ValueError:\n",
    "                print(\"Error: Unable to extract image numbers from file names.\")\n",
    "                print(\"Please ensure that file names are in the format 'number.jpg'\")\n",
    "                continue\n",
    "        else:\n",
    "            image_number = 1\n",
    "\n",
    "        # Save the image to the corresponding folder\n",
    "        image_path = os.path.join(folder_path, f'{image_number}.jpg')\n",
    "        cv2.imwrite(image_path, frame)\n",
    "        print(f\"Image saved as {image_path}\")\n",
    "\n",
    "    # Release the webcam and close OpenCV windows\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Number of images to capture\n",
    "num_images = 10\n",
    "\n",
    "# Location of the data directory\n",
    "data_dir = r'C:\\Users\\acer1\\Downloads\\ai&ml\\Sign lang\\sign lang img data'\n",
    "\n",
    "# Call the function to capture and save images\n",
    "capture_and_save_images(num_images, data_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c08f2afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 55500 images belonging to 37 classes.\n",
      "Found 55500 images belonging to 37 classes.\n",
      "Found 0 images belonging to 37 classes.\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node categorical_crossentropy/softmax_cross_entropy_with_logits defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"C:\\Users\\acer1\\anaconda3\\Lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n\n  File \"C:\\Users\\acer1\\anaconda3\\Lib\\site-packages\\traitlets\\config\\application.py\", line 992, in launch_instance\n\n  File \"C:\\Users\\acer1\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 736, in start\n\n  File \"C:\\Users\\acer1\\anaconda3\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n\n  File \"C:\\Users\\acer1\\anaconda3\\Lib\\asyncio\\base_events.py\", line 607, in run_forever\n\n  File \"C:\\Users\\acer1\\anaconda3\\Lib\\asyncio\\base_events.py\", line 1922, in _run_once\n\n  File \"C:\\Users\\acer1\\anaconda3\\Lib\\asyncio\\events.py\", line 80, in _run\n\n  File \"C:\\Users\\acer1\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 516, in dispatch_queue\n\n  File \"C:\\Users\\acer1\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 505, in process_one\n\n  File \"C:\\Users\\acer1\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 412, in dispatch_shell\n\n  File \"C:\\Users\\acer1\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 740, in execute_request\n\n  File \"C:\\Users\\acer1\\anaconda3\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 422, in do_execute\n\n  File \"C:\\Users\\acer1\\anaconda3\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 546, in run_cell\n\n  File \"C:\\Users\\acer1\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3024, in run_cell\n\n  File \"C:\\Users\\acer1\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3079, in _run_cell\n\n  File \"C:\\Users\\acer1\\anaconda3\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"C:\\Users\\acer1\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3284, in run_cell_async\n\n  File \"C:\\Users\\acer1\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3466, in run_ast_nodes\n\n  File \"C:\\Users\\acer1\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3526, in run_code\n\n  File \"C:\\Users\\acer1\\AppData\\Local\\Temp\\ipykernel_30668\\3974419738.py\", line 74, in <module>\n\n  File \"C:\\Users\\acer1\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n\n  File \"C:\\Users\\acer1\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1807, in fit\n\n  File \"C:\\Users\\acer1\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function\n\n  File \"C:\\Users\\acer1\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1384, in step_function\n\n  File \"C:\\Users\\acer1\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1373, in run_step\n\n  File \"C:\\Users\\acer1\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1151, in train_step\n\n  File \"C:\\Users\\acer1\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1209, in compute_loss\n\n  File \"C:\\Users\\acer1\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\compile_utils.py\", line 277, in __call__\n\n  File \"C:\\Users\\acer1\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py\", line 143, in __call__\n\n  File \"C:\\Users\\acer1\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py\", line 270, in call\n\n  File \"C:\\Users\\acer1\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py\", line 2221, in categorical_crossentropy\n\n  File \"C:\\Users\\acer1\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py\", line 5579, in categorical_crossentropy\n\nlogits and labels must be broadcastable: logits_size=[32,35] labels_size=[32,37]\n\t [[{{node categorical_crossentropy/softmax_cross_entropy_with_logits}}]] [Op:__inference_train_function_9653]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 74\u001b[0m\n\u001b[0;32m     64\u001b[0m test_generator \u001b[38;5;241m=\u001b[39m datagen\u001b[38;5;241m.\u001b[39mflow_from_directory(\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:/Users/acer1/Downloads/ai&ml/Sign lang/Gesture Image Data\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     66\u001b[0m     target_size\u001b[38;5;241m=\u001b[39mimage_size,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m     subset\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Use the validation subset of the data\u001b[39;00m\n\u001b[0;32m     71\u001b[0m )\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 74\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m     75\u001b[0m     train_generator,\n\u001b[0;32m     76\u001b[0m     steps_per_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(X_train) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m batch_size,\n\u001b[0;32m     77\u001b[0m     epochs\u001b[38;5;241m=\u001b[39mepochs,\n\u001b[0;32m     78\u001b[0m     validation_data\u001b[38;5;241m=\u001b[39mtest_generator,\n\u001b[0;32m     79\u001b[0m     validation_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(X_test) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m batch_size\n\u001b[0;32m     80\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node categorical_crossentropy/softmax_cross_entropy_with_logits defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"C:\\Users\\acer1\\anaconda3\\Lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n\n  File \"C:\\Users\\acer1\\anaconda3\\Lib\\site-packages\\traitlets\\config\\application.py\", line 992, in launch_instance\n\n  File \"C:\\Users\\acer1\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 736, in start\n\n  File \"C:\\Users\\acer1\\anaconda3\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n\n  File \"C:\\Users\\acer1\\anaconda3\\Lib\\asyncio\\base_events.py\", line 607, in run_forever\n\n  File \"C:\\Users\\acer1\\anaconda3\\Lib\\asyncio\\base_events.py\", line 1922, in _run_once\n\n  File \"C:\\Users\\acer1\\anaconda3\\Lib\\asyncio\\events.py\", line 80, in _run\n\n  File \"C:\\Users\\acer1\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 516, in dispatch_queue\n\n  File \"C:\\Users\\acer1\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 505, in process_one\n\n  File \"C:\\Users\\acer1\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 412, in dispatch_shell\n\n  File \"C:\\Users\\acer1\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 740, in execute_request\n\n  File \"C:\\Users\\acer1\\anaconda3\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 422, in do_execute\n\n  File \"C:\\Users\\acer1\\anaconda3\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 546, in run_cell\n\n  File \"C:\\Users\\acer1\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3024, in run_cell\n\n  File \"C:\\Users\\acer1\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3079, in _run_cell\n\n  File \"C:\\Users\\acer1\\anaconda3\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"C:\\Users\\acer1\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3284, in run_cell_async\n\n  File \"C:\\Users\\acer1\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3466, in run_ast_nodes\n\n  File \"C:\\Users\\acer1\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3526, in run_code\n\n  File \"C:\\Users\\acer1\\AppData\\Local\\Temp\\ipykernel_30668\\3974419738.py\", line 74, in <module>\n\n  File \"C:\\Users\\acer1\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n\n  File \"C:\\Users\\acer1\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1807, in fit\n\n  File \"C:\\Users\\acer1\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function\n\n  File \"C:\\Users\\acer1\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1384, in step_function\n\n  File \"C:\\Users\\acer1\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1373, in run_step\n\n  File \"C:\\Users\\acer1\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1151, in train_step\n\n  File \"C:\\Users\\acer1\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1209, in compute_loss\n\n  File \"C:\\Users\\acer1\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\compile_utils.py\", line 277, in __call__\n\n  File \"C:\\Users\\acer1\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py\", line 143, in __call__\n\n  File \"C:\\Users\\acer1\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py\", line 270, in call\n\n  File \"C:\\Users\\acer1\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py\", line 2221, in categorical_crossentropy\n\n  File \"C:\\Users\\acer1\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py\", line 5579, in categorical_crossentropy\n\nlogits and labels must be broadcastable: logits_size=[32,35] labels_size=[32,37]\n\t [[{{node categorical_crossentropy/softmax_cross_entropy_with_logits}}]] [Op:__inference_train_function_9653]"
     ]
    }
   ],
   "source": [
    "#training same model with different datase\n",
    "#training the model\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.applications import MobileNet\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
    "\n",
    "# Parameters\n",
    "image_size = (224, 224)\n",
    "num_classes = 35  # 10 digits + 26 letters\n",
    "batch_size = 32\n",
    "epochs = 5\n",
    "\n",
    "# Load MobileNet model without top layers\n",
    "base_model = MobileNet(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Freeze base model layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add custom top layers for classification\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "# Load and preprocess the data\n",
    "data_generator = datagen.flow_from_directory(\n",
    "    'C:/Users/acer1/Downloads/ai&ml/Sign lang/Gesture Image Data',\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False  # Ensure data isn't shuffled so train-test split is consistent\n",
    ")\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_generator.filenames, data_generator.classes, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train generator\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    'C:/Users/acer1/Downloads/ai&ml/Sign lang/Gesture Image Data',\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False,\n",
    "    subset='training'  # Use the training subset of the data\n",
    ")\n",
    "\n",
    "# Test generator\n",
    "test_generator = datagen.flow_from_directory(\n",
    "    'C:/Users/acer1/Downloads/ai&ml/Sign lang/Gesture Image Data',\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False,\n",
    "    subset='validation'  # Use the validation subset of the data\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(X_train) // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=test_generator,\n",
    "    validation_steps=len(X_test) // batch_size\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c1ed8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
